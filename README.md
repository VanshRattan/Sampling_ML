# ðŸ“˜ Sampling Assignment â€“ Credit Card Fraud Detection
**Student ID:** 102316118

---

## ðŸŽ¯ Quick Overview (1-Minute Summary)

**What:** Compare 5 sampling techniques across 5 ML models on a credit card fraud dataset.

**Why:** Dataset is imbalanced (fraud cases << normal cases). Need fair model training.

**How:** Balance data â†’ Apply 5 sampling techniques â†’ Train 5 models â†’ Compare results.

---

## ï¿½ The Process

**Step 1** â†’ **Step 2** â†’ **Step 3** â†’ **Step 4** â†’ **Step 5** â†’ **Step 6**

| Step | Action | Details |
|------|--------|---------|
| 1ï¸âƒ£ | Load Dataset | Import credit card data (imbalanced: 99.8% normal, 0.2% fraud) |
| 2ï¸âƒ£ | Balance Data | Oversample fraud cases until 50-50 split |
| 3ï¸âƒ£ | Apply 5 Sampling | Simple Random, Stratified, Bootstrap, K-Fold, Stratified K-Fold |
| 4ï¸âƒ£ | Train 5 Models | LogReg, KNN, Decision Tree, Random Forest, SVM |
| 5ï¸âƒ£ | Analyze Results | Create accuracy table and visualization |
| 6ï¸âƒ£ | Conclusion | Find best sampling technique for each model |

---

## ðŸ“ˆ Key Results

| Model | Best Sampling | Accuracy |
|-------|--------------|----------|
| **Logistic Regression** | Sampling4 | 97.47% |
| **KNN** | Sampling1 | 96.86% |
| **Decision Tree** | Sampling1 | 97.94% |
| **Random Forest** | Sampling1 | 97.65% |
| **SVM** | Sampling1 | 96.55% |

**Winner:** Sampling1 (Simple Random) works best for 4/5 models! â­

---

## ðŸ”„ 5 Sampling Techniques Explained

| # | Technique | How It Works | Best For |
|---|-----------|-------------|----------|
| **1** | Simple Random | Random 75-25 split | Non-linear models, trees |
| **2** | Stratified | Split while keeping 50-50 ratio | Maintaining balance |
| **3** | Bootstrap | Sample with replacement | Reducing variance |
| **4** | K-Fold | Divide into 5 folds | Logistic Regression |
| **5** | Stratified K-Fold | K-Fold + balanced splits | Robust validation |

---

## ðŸ’¡ Why Sampling1 Wins

âœ… Data already balanced â†’ No need for complex techniques  
âœ… Simple = Fewer errors & faster  
âœ… Works great with decision trees & random forests  
âœ… Preserves natural data structure  

**Exception:** Logistic Regression prefers Sampling4 (K-Fold) for stable decision boundaries.

---

## ðŸ“ Files

- `102316118.ipynb` â€“ All code & experiments
- `Creditcard_data.csv` â€“ Dataset
- `README.md` â€“ This guide

---

## âœ… What We Learned

1. **Balance first** â€“ Imbalanced data = biased models
2. **Simple > Complex** â€“ When data is balanced, don't overcomplicate
3. **Model matters** â€“ Linear vs non-linear models prefer different techniques
4. **No one-size-fits-all** â€“ Test & choose based on your data & model type

**Final Takeaway:** For balanced datasets, use **Simple Random Sampling (Sampling1)** for best results with most ML models!

--  
