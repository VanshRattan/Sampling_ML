# ðŸ“˜ Sampling Assignment â€“ Credit Card Fraud Detection
**Student ID:** 102316118

---

## ðŸŽ¯ Quick Overview (1-Minute Summary)

**What:** Compare 5 sampling techniques across 5 ML models on a credit card fraud dataset.

**Why:** Dataset is imbalanced (fraud cases << normal cases). Need fair model training.

**How:** Balance data â†’ Apply 5 sampling techniques â†’ Train 5 models â†’ Compare results.

---

## ï¿½ The Process

```
1. Load Dataset           2. Balance Data           3. Apply 5 Sampling
   â†“                        â†“                         Techniques
   Imbalanced           Oversample Fraud              â†“
   (99.8% Normal)       (50-50 Split)            4. Train 5 Models
                                                     on Each Sample
                                                     â†“
6. Conclusion        5. Analyze Results        
   â†“                      â†“
Sampling1 BEST!    Results Table + Graph
(for 4/5 models)    Accuracy Comparison
```

---

## ðŸ“ˆ Key Results

| Model | Best Sampling | Accuracy |
|-------|--------------|----------|
| **Logistic Regression** | Sampling4 | 97.47% |
| **KNN** | Sampling1 | 96.86% |
| **Decision Tree** | Sampling1 | 97.94% |
| **Random Forest** | Sampling1 | 97.65% |
| **SVM** | Sampling1 | 96.55% |

**Winner:** Sampling1 (Simple Random) works best for 4/5 models! â­

---

## ðŸ”„ 5 Sampling Techniques Explained

| # | Technique | How It Works | Best For |
|---|-----------|-------------|----------|
| **1** | Simple Random | Random 75-25 split | Non-linear models, trees |
| **2** | Stratified | Split while keeping 50-50 ratio | Maintaining balance |
| **3** | Bootstrap | Sample with replacement | Reducing variance |
| **4** | K-Fold | Divide into 5 folds | Logistic Regression |
| **5** | Stratified K-Fold | K-Fold + balanced splits | Robust validation |

---

## ðŸ’¡ Why Sampling1 Wins

âœ… Data already balanced â†’ No need for complex techniques  
âœ… Simple = Fewer errors & faster  
âœ… Works great with decision trees & random forests  
âœ… Preserves natural data structure  

**Exception:** Logistic Regression prefers Sampling4 (K-Fold) for stable decision boundaries.

---

## ðŸ“ Files

- `102316118.ipynb` â€“ All code & experiments
- `Creditcard_data.csv` â€“ Dataset
- `README.md` â€“ This guide

---

## âœ… What We Learned

1. **Balance first** â€“ Imbalanced data = biased models
2. **Simple > Complex** â€“ When data is balanced, don't overcomplicate
3. **Model matters** â€“ Linear vs non-linear models prefer different techniques
4. **No one-size-fits-all** â€“ Test & choose based on your data & model type

**Final Takeaway:** For balanced datasets, use **Simple Random Sampling (Sampling1)** for best results with most ML models!

--  
